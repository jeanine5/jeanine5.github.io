---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---

<div style="margin-top: 20px;">

  <h2>Research Focus</h2>
  <p>Iâ€™m interested in building efficient machine learning infrastructure, especially at the intersection of <strong>low-level systems</strong> and <strong>model optimization</strong>.</p>

  <h3>Current & Future Research Goals</h3>
  <ul>
    <li>Exploring compiler/runtime-level optimizations for GPU compute</li>
    <li>Designing hardware/software co-optimized training pipelines</li>
    <li>Seeking opportunities in ML acceleration and systems research during my time at UC Davis and Bay Area</li>
  </ul>

  <h3>Past Research Projects</h3>

  <ul>
    <li>
      <strong>LLM Distillation Pipeline (NeurIPS 2025 submission)</strong><br>
      I, along with a grad student and several undergrad interns, developed a novel knowledge distillation method called <strong>TuneShift-KD</strong> to transfer specialized knowledge between large language models when the original fine-tuning data is unavailable.
      The key insight of the method is to identify prompts where the fine-tuned model exhibits lower perplexity than its base model, signaling domain-specific knowledge.
      These high-gain prompts are expanded iteratively to build a synthetic dataset for distillation.
      TuneShift-KD is fully automated, requires no access to original data, and outperforms previous approaches in preserving specialized performance.<br>
      <a href="https://openreview.net/forum?id=VBFcEJOYV0" target="_blank">Overview</a>
    </li>

    <li>
      <strong>Vision Science & Facial Expression Representation</strong><br>
      This project investigated how individual conceptual knowledge shapes visual representations of facial expressions across six basic emotions and pain.
      Using a novel combination of reverse correlation and a genetic algorithm, we efficiently generated photorealistic avatars expressing participant-specific affective patterns.
      Participants completed a conceptual similarity task and a visual reverse correlation task, enabling the construction of individual-level matrices that captured conceptual and perceptual structure.
      Analyses revealed significant correlations between these matrices, providing robust evidence for a link between how people think about emotions and how they visually perceive them.
      The results highlight meaningful individual differences in this relationship, advancing our understanding of the cognitive-emotional basis of facial expression interpretation.<br>
      <a href="https://drive.google.com/file/d/1TRB49jOXAaKSP-bTTZui1nVUcSWSVsFQ/view" target="_blank">Poster Presentation</a>
    </li>

    <li>
      <strong>Body Motion & ML with Prof. David Rokeby</strong><br>
      I worked on training and evolving Variational Autoencoders (VAEs) for the AMASS human motion dataset using genetic algorithms.
      The models compressed joint quaternion data into latent representations, and I implemented mutation and crossover strategies tailored to VAE structure for optimization.
      The project explored architectural search under multi-objective constraints such as loss and potential out-of-distribution generalization.
      I also designed preprocessing pipelines to reshape 3D motion into flattened, trainable formats and built an evaluation workflow to compare VAE performance during training.<br>
      <a href="https://github.com/jeanine5/VAE-NAS" target="_blank">Repo</a>
    </li>
  </ul>

</div>
